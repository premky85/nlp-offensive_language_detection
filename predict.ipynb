{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models.models import TextClassificationNet\n",
    "from models.utils.data_loader import TextDataloader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(data, model_name, weights_path):\n",
    "    ''' \n",
    "    Predict function for text classification\n",
    "\n",
    "    ----------------\n",
    "    INPUTS:\n",
    "    data: Pandas.DataFrame - text column named 'text', label column named 'class' with possible values [0, 1, 2] (0 - normal language, 1 - hate speech, 2 - offensive language)\n",
    "    model_name: String - possible values ['bert', 'csebert', 'xlmr']\n",
    "    weights_path: String - path to weights file\n",
    "\n",
    "    ----------------\n",
    "    OUTPUTS:\n",
    "    preds: List - list of softmax predictions for each input text\n",
    "    '''\n",
    "    preds = []\n",
    "\n",
    "    text_dataloader = TextDataloader(seq_length=128, language_model=model_name)\n",
    "    tcn = TextClassificationNet(seq_length=128, language_model=model_name)\n",
    "    model = tcn.get_model()\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    bs = 64\n",
    "    with tqdm(total=data.shape[0] // bs) as pbar:\n",
    "        for i,rows in data.groupby(np.arange(len(data))//bs):\n",
    "            pbar.update(1)\n",
    "            tokens, masks = zip(*map(text_dataloader.tokenize, rows['text'].to_numpy()))\n",
    "            if len(masks) > 1 and len(tokens) > 1:\n",
    "                tokens, masks = tf.squeeze(tf.stack(tokens)), tf.squeeze(tf.stack(masks))\n",
    "\n",
    "            predicted = model.predict([tokens, masks])\n",
    "            preds += list(predicted)\n",
    "\n",
    "    return preds\n",
    "\n",
    "def evaluate(labels, predictions, name='generic_network'):\n",
    "    print('=================== ' + name + ' ===================\\n')\n",
    "    print(confusion_matrix(labels, predictions))\n",
    "    print(classification_report(labels, predictions, digits=3))\n",
    "    print('\\n\\n')\n",
    "\n",
    "def ensemble_voting(model_preds):\n",
    "    ''' \n",
    "    Combine predictions for ensemble voting\n",
    "\n",
    "    ----------------\n",
    "    INPUTS:\n",
    "    model_preds: List - 3d list of softmax predictions for each model contributing to voting\n",
    "\n",
    "    ----------------\n",
    "    OUTPUTS:\n",
    "    preds: List - 1d list of predictions\n",
    "    '''\n",
    "\n",
    "    return (np.sum(model_preds, axis=0) / len(model_preds)).argmax(axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create Pandas DataFrame with text column named 'text' and labels column names 'class\n",
    "\n",
    "df = None\n",
    "\n",
    "# base = '/path/to/folder/with/preprocessed/data/'\n",
    "\n",
    "'''\n",
    "If you wish to split training data to train and test set uncomment following lines:\n",
    "'''\n",
    "\n",
    "# path = [base + 'fox_news_comments_preprocessed.csv', base + 'gab_preprocessed.csv', base + 'reddit_preprocessed.csv', base + 'white_supremacist_forum_preprocessed.csv', base + 'twitter_preprocessed.csv', base + 'slo_twitter_preprocessed.csv']\n",
    "# text_column_names = ['text', 'text', 'text', 'text', 'tweet', 'tweet']\n",
    "# label_column_names = ['class', 'class', 'class', 'class', 'class', 'class']\n",
    "# dataloader = TextDataloader(seq_length=SEQ_LENGTH)\n",
    "# dataloader.load_data(path, text_column_name=text_column_names, label_column_name=label_column_names)\n",
    "# df = dataloader.get_test_data() \n",
    "\n",
    "'''\n",
    "If you wish to use single .csv file for testing uncomment following line:\n",
    "'''\n",
    "\n",
    "# df = pd.read_csv(base + 'slo_test_data_preprocessed.csv')\n",
    "\n",
    "labels = df['class'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert_predictions = predict(df, model_name='bert', weights_path='/path/to/weights.h5')\n",
    "csebert_predictions = predict(df, model_name='csebert', weights_path='/path/to/weights.h5')\n",
    "xlmr_predictions = predict(df, model_name='xlmr', weights_path='/path/to/weights.h5')\n",
    "\n",
    "ensemble_preds = [mbert_predictions, csebert_predictions, xlmr_predictions]\n",
    "evaluate(labels, ensemble_preds, 'Ensemble')\n",
    "\n",
    "mbert_preds = mbert_predictions.argmax(axis=-1)\n",
    "evaluate(labels, mbert_preds, 'mBERT')\n",
    "\n",
    "csebert_preds = csebert_predictions.argmax(axis=-1)\n",
    "evaluate(labels, csebert_preds, 'CroSloEngualBERT')\n",
    "\n",
    "xlmr_preds = xlmr_predictions.argmax(axis=-1)\n",
    "evaluate(labels, xlmr_preds, 'XLMr')\n"
   ]
  }
 ]
}